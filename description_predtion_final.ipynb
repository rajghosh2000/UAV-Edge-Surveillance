{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3lGn5oVuTb8",
        "outputId": "a07d6ab1-769d-478c-df1f-4dd9983a4d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Drive re-mounted successfully.\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Re-mount the drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Drive re-mounted successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XCzbRhIcpZ_8",
        "outputId": "6b111635-2993-4562-9df9-fb21872480a0"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -q transformers datasets accelerate pillow nltk\n",
        "\n",
        "import os, json, types, inspect, random\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "import torch\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True) \n",
        "print(\" Libraries ready.\")\n",
        "\n",
        "\n",
        "# Paths\n",
        "json_path = \"/content/drive/MyDrive/blip_model/final_train_description.json\"\n",
        "base_image_dir = \"/content/drive/MyDrive/blip_model\"\n",
        "output_dir = \"/content/drive/MyDrive/blip_model/blip_scene_finetuned_fulldata_2\"\n",
        "\n",
        "test_results_path = os.path.join(output_dir, \"test_results.json\")\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(\"Paths configured.\")\n",
        "\n",
        "model_name = \"Salesforce/blip-image-captioning-base\"\n",
        "processor = BlipProcessor.from_pretrained(model_name)\n",
        "model = BlipForConditionalGeneration.from_pretrained(model_name)\n",
        "print(\"BLIP model loaded.\")\n",
        "\n",
        "# Dataset Class\n",
        "class SceneUnderstandingDataset(Dataset):\n",
        "    def __init__(self, items, base_dir, processor, max_len=64):\n",
        "        self.base_dir = base_dir\n",
        "        self.processor = processor\n",
        "        self.max_len = max_len\n",
        "\n",
        "        valid = []\n",
        "        skipped = 0\n",
        "        for item in items:\n",
        "            img_path = os.path.join(base_dir, item[\"image\"])\n",
        "            if not os.path.exists(img_path):\n",
        "                skipped += 1\n",
        "                continue\n",
        "            try:\n",
        "                with Image.open(img_path) as im:\n",
        "                    im.verify()\n",
        "            except Exception:\n",
        "                skipped += 1\n",
        "                continue\n",
        "            valid.append(item)\n",
        "        self.data = valid\n",
        "        print(f\"Dataset ready: {len(self.data)} valid, skipped {skipped}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        img_path = os.path.join(self.base_dir, item[\"image\"])\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "        except Exception:\n",
        "            image = Image.new(\"RGB\", (224,224), (0,0,0))\n",
        "            caption = \"unreadable image\"\n",
        "        else:\n",
        "            caption = item.get(\"caption\", \"\")\n",
        "\n",
        "        enc = self.processor(\n",
        "            images=image,\n",
        "            text=caption,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "        )\n",
        "\n",
        "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
        "        labels = input_ids.clone()\n",
        "        pad_id = self.processor.tokenizer.pad_token_id or 0\n",
        "        labels[labels == pad_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": enc[\"pixel_values\"].squeeze(0),\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"labels\": labels,\n",
        "            \"caption\": caption,\n",
        "            \"image_file\": item[\"image\"],\n",
        "        }\n",
        "\n",
        "# Load JSON & Split\n",
        "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "random.shuffle(data)\n",
        "split = int(0.9 * len(data))\n",
        "train_items, test_items = data[:split], data[split:]\n",
        "\n",
        "train_dataset = SceneUnderstandingDataset(train_items, base_image_dir, processor)\n",
        "test_dataset = SceneUnderstandingDataset(test_items, base_image_dir, processor)\n",
        "\n",
        "# Training Arguments\n",
        "desired_args = {\n",
        "    \"output_dir\": output_dir,\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 4,\n",
        "    \"num_train_epochs\": 6,\n",
        "    \"learning_rate\": 5e-6,\n",
        "    \"weight_decay\": 0.02,\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"save_strategy\": \"epoch\",\n",
        "    \"evaluation_strategy\": \"epoch\", \n",
        "    \"save_total_limit\": 2,\n",
        "    \"logging_steps\": 10,\n",
        "    \"load_best_model_at_end\": True,\n",
        "    \"metric_for_best_model\": \"eval_loss\",\n",
        "    \"remove_unused_columns\": False,\n",
        "    \"fp16\": torch.cuda.is_available(),\n",
        "    \"report_to\": \"none\",\n",
        "}\n",
        "\n",
        "# Filter keys by version\n",
        "ta_params = inspect.signature(TrainingArguments.__init__).parameters\n",
        "supported = set(ta_params.keys()) - {\"self\", \"kwargs\"}\n",
        "filtered_args = {k: v for k, v in desired_args.items() if k in supported}\n",
        "\n",
        "# Safety Fix: Disable load_best_model if no eval strategy\n",
        "if \"evaluation_strategy\" not in supported:\n",
        "    filtered_args.pop(\"load_best_model_at_end\", None)\n",
        "    filtered_args.pop(\"metric_for_best_model\", None)\n",
        "    print(\"No evaluation strategy in this transformers version — disabled best model loading.\")\n",
        "\n",
        "training_args = TrainingArguments(**filtered_args)\n",
        "print(\"TrainingArguments created successfully.\")\n",
        "\n",
        "# Patch BLIP forward\n",
        "def patched_forward(self, *args, **kwargs):\n",
        "    kwargs.pop(\"num_items_in_batch\", None)\n",
        "    return self.old_forward(*args, **kwargs)\n",
        "\n",
        "model.old_forward = model.forward\n",
        "model.forward = types.MethodType(patched_forward, model)\n",
        "print(\"Patched model.forward().\")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset if len(test_dataset) > 0 else None,\n",
        ")\n",
        "\n",
        "# Train\n",
        "print(\"Training...\")\n",
        "trainer.train()\n",
        "print(\"Training done.\")\n",
        "\n",
        "# Save\n",
        "model.save_pretrained(output_dir)\n",
        "processor.save_pretrained(output_dir)\n",
        "print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "# Evaluation (BLEU)\n",
        "if len(test_dataset) > 0:\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model_eval = BlipForConditionalGeneration.from_pretrained(output_dir).to(device)\n",
        "    model_eval.eval()\n",
        "\n",
        "    def bleu(ref, hyp):\n",
        "        ref_tokens = [nltk.word_tokenize(ref.lower())]\n",
        "        hyp_tokens = nltk.word_tokenize(hyp.lower())\n",
        "        return sentence_bleu(ref_tokens, hyp_tokens, weights=(0.25,0.25,0.25,0.25),\n",
        "                             smoothing_function=SmoothingFunction().method1)\n",
        "\n",
        "    total_bleu, count = 0, 0\n",
        "    results = []\n",
        "\n",
        "    for s in test_dataset:\n",
        "        try:\n",
        "            pv = s[\"pixel_values\"].unsqueeze(0).to(device)\n",
        "            gt = s[\"caption\"]\n",
        "            with torch.no_grad():\n",
        "                out = model_eval.generate(pixel_values=pv, max_length=64, num_beams=3)\n",
        "            gen = processor.decode(out[0], skip_special_tokens=True)\n",
        "            score = bleu(gt, gen)\n",
        "            total_bleu += score\n",
        "            count += 1\n",
        "            results.append({\n",
        "                \"image_file\": s[\"image_file\"],\n",
        "                \"ground_truth\": gt,\n",
        "                \"generated\": gen,\n",
        "                \"bleu\": score\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(\"Eval error:\", e)\n",
        "            continue\n",
        "\n",
        "    if count > 0:\n",
        "        avg_bleu = total_bleu / count\n",
        "        print(f\"\\nAverage BLEU-4 ({count} samples): {avg_bleu:.4f}\")\n",
        "        # Saving results to the specified path: test_results_path\n",
        "        with open(test_results_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(results, f, indent=2)\n",
        "        print(f\"Results saved to {test_results_path}\")\n",
        "    else:\n",
        "        print(\"No valid test samples.\")\n",
        "else:\n",
        "    print(\"Test dataset empty; skipping evaluation.\")\n",
        "\n",
        "print(\"All done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coZKBcR6rfh3",
        "outputId": "96a96941-e359-4326-bb24-f756cd513d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Google Drive mounted.\n",
            "✅ Fine-tuned model loaded successfully from /content/drive/MyDrive/blip_model/blip_scene_finetuned_fulldata_2 to cuda.\n",
            "\n",
            "🚀 Generating caption...\n",
            "\n",
            "--- Generation Result ---\n",
            "Image File: test_image_5.jpg\n",
            "Generated Caption: **a low - angle shot of a person falling from a high - rise building facing towards the ground**\n",
            "-------------------------\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import torch\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/blip_model/blip_scene_finetuned_fulldata_2\"\n",
        "base_image_dir = \"/content/drive/MyDrive/blip_model\"\n",
        "\n",
        "test_image_file = \"test_image_5.jpg\"\n",
        "test_image_path = os.path.join(base_image_dir, test_image_file)\n",
        "\n",
        "if not os.path.exists(test_image_path):\n",
        "    print(f\"Error: Test image not found at {test_image_path}.\")\n",
        "    print(\"Please update 'test_image_file' to an existing image name in your Google Drive.\")\n",
        "else:\n",
        "    try:\n",
        "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        processor = BlipProcessor.from_pretrained(output_dir)\n",
        "        model = BlipForConditionalGeneration.from_pretrained(output_dir).to(device)\n",
        "        model.eval()\n",
        "        print(f\"Fine-tuned model loaded successfully from {output_dir} to {device}.\")\n",
        "\n",
        "        image = Image.open(test_image_path).convert(\"RGB\")\n",
        "\n",
        "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        print(\"\\nGenerating caption...\")\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(\n",
        "                pixel_values=inputs.pixel_values,\n",
        "                max_length=64,\n",
        "                num_beams=4\n",
        "            )\n",
        "\n",
        "        # Decode the generated IDs to a readable string\n",
        "        generated_caption = processor.decode(generated_ids.squeeze(0), skip_special_tokens=True)\n",
        "\n",
        "        # --- Output Result ---\n",
        "        print(\"\\n--- Generation Result ---\")\n",
        "        print(f\"Image File: {test_image_file}\")\n",
        "        print(f\"Generated Caption: {generated_caption}\")\n",
        "        print(\"-------------------------\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during testing. Check if all required files (config.json, pytorch_model.bin, preprocessor_config.json) are in {output_dir}. Error: {e}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
